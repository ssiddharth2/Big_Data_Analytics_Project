{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 6.8 Model Building\n",
    "\n",
    "Nikki Aaron (na5zn), Kevin Hoffman (keh4nb), Ashley Scurlock (ams5zx), Siddharth Surapaneni (sss2ea)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- zip: integer (nullable = true)\n",
      " |-- lat: double (nullable = true)\n",
      " |-- lng: double (nullable = true)\n",
      " |-- timezone: string (nullable = true)\n",
      " |-- county_name: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- state_id: string (nullable = true)\n",
      " |-- state_name: string (nullable = true)\n",
      " |-- population: double (nullable = true)\n",
      " |-- density: double (nullable = true)\n",
      " |-- res_rate: double (nullable = true)\n",
      " |-- annual_mbtu_used: double (nullable = true)\n",
      " |-- annual_kwh_used: double (nullable = true)\n",
      " |-- temp_Apr: double (nullable = true)\n",
      " |-- temp_Aug: double (nullable = true)\n",
      " |-- temp_Dec: double (nullable = true)\n",
      " |-- temp_Feb: double (nullable = true)\n",
      " |-- temp_Jan: double (nullable = true)\n",
      " |-- temp_Jul: double (nullable = true)\n",
      " |-- temp_Jun: double (nullable = true)\n",
      " |-- temp_Mar: double (nullable = true)\n",
      " |-- temp_May: double (nullable = true)\n",
      " |-- temp_Nov: double (nullable = true)\n",
      " |-- temp_Oct: double (nullable = true)\n",
      " |-- temp_Sep: double (nullable = true)\n",
      " |-- pct_cloudy_days_Apr: double (nullable = true)\n",
      " |-- pct_cloudy_days_Aug: double (nullable = true)\n",
      " |-- pct_cloudy_days_Dec: double (nullable = true)\n",
      " |-- pct_cloudy_days_Feb: double (nullable = true)\n",
      " |-- pct_cloudy_days_Jan: double (nullable = true)\n",
      " |-- pct_cloudy_days_Jul: double (nullable = true)\n",
      " |-- pct_cloudy_days_Jun: double (nullable = true)\n",
      " |-- pct_cloudy_days_Mar: double (nullable = true)\n",
      " |-- pct_cloudy_days_May: double (nullable = true)\n",
      " |-- pct_cloudy_days_Nov: double (nullable = true)\n",
      " |-- pct_cloudy_days_Oct: double (nullable = true)\n",
      " |-- pct_cloudy_days_Sep: double (nullable = true)\n",
      " |-- annual_output_w_hrs: double (nullable = true)\n",
      " |-- dni: double (nullable = true)\n",
      " |-- annual_output_kwh_20_sps: double (nullable = true)\n",
      " |-- percent_current_needs_met: double (nullable = true)\n",
      " |-- dollars_saved: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "# Load training data\n",
    "df = spark.read.csv(\"data_combined.csv\", inferSchema =True, header=True)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(zip=1002, lat=42.37492, lng=-72.4621, population=30014.0, density=210.5, annual_mbtu_used=63.0, annual_kwh_used=18463.47741, temp_Apr=61.05, temp_Aug=80.35, temp_Dec=36.25, temp_Feb=22.95, temp_Jan=30.8, temp_Jul=85.25, temp_Jun=80.6, temp_Mar=39.9, temp_May=64.9, temp_Nov=45.75, temp_Oct=56.2, temp_Sep=79.1, pct_cloudy_days_Apr=70.85, pct_cloudy_days_Aug=63.3, pct_cloudy_days_Dec=69.15, pct_cloudy_days_Feb=67.9, pct_cloudy_days_Jan=64.3, pct_cloudy_days_Jul=63.85, pct_cloudy_days_Jun=67.0, pct_cloudy_days_Mar=73.9, pct_cloudy_days_May=68.80000000000001, pct_cloudy_days_Nov=73.6, pct_cloudy_days_Oct=66.05, pct_cloudy_days_Sep=63.9, dni=4.470000075000001, annual_output_kwh_20_sps=3042.215795183656, dollars_saved=516.1589089424086)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select columns for training\n",
    "label = \"dollars_saved\"\n",
    "\n",
    "feature_to_ignore = [\"res_rate\", \"annual_output_w_hrs\", \"anuual_output_kwh20_sps\", \"percent_current_needs_met\", \"country_name\", \"county_name\", \"city\", \"state_id\", \"state_name\", \"timezone\"]\n",
    "features = [c for c in df.columns if c not in feature_to_ignore + [label]]\n",
    "\n",
    "df_clean = df.select([c for c in df.columns if c not in feature_to_ignore])\n",
    "df_train, df_test = df_clean.randomSplit([0.8, 0.2], 500)\n",
    "df_train.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1: Linear Regression\n",
    "Best hyperparameters: maxIter:100, regParam:0, elasticNetParam: 0\n",
    "\n",
    "Model Size: 148 KB\n",
    "\n",
    "RMSE: 65.9148"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler, OneHotEncoder, StandardScaler\n",
    "from pyspark.ml import Pipeline \n",
    "\n",
    "assembler_features = VectorAssembler(\n",
    "    inputCols=features,\n",
    "    outputCol=\"features\")\n",
    "\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\")\n",
    "\n",
    "lr = LinearRegression(maxIter=100, labelCol=label, featuresCol=\"scaled_features\", regParam=0.0, elasticNetParam=0.0,)\n",
    "\n",
    "# Build the pipeline\n",
    "pipeline = Pipeline(stages=[assembler_features, scaler, lr])\n",
    "\n",
    "# Fit the pipeline\n",
    "model = pipeline.fit(df_train)\n",
    "#model.save(\"model-regression\")\n",
    "# Make a prediction\n",
    "prediction = model.transform(df_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------------------+\n",
      "|    dollars_saved|        prediction|\n",
      "+-----------------+------------------+\n",
      "|484.5031021125742| 474.8286683709772|\n",
      "|484.6682391251149|456.72195690845274|\n",
      "|516.8557923471669|465.00491251832386|\n",
      "|516.0491487817686|449.00953778037854|\n",
      "|516.0173890931666|  435.398548433369|\n",
      "|515.7661149761013| 479.3164287258172|\n",
      "|515.8825341130485|474.49442316161293|\n",
      "| 515.864331864375|434.12566696947437|\n",
      "|516.4802310303666| 487.3332139283924|\n",
      "|515.7580085695923| 434.7944581590876|\n",
      "|484.4272456962049|473.09527872285526|\n",
      "|516.0233877856182|435.57289012456744|\n",
      "|516.4444948751209| 443.1325149456666|\n",
      "|516.3146217883103| 435.1499450895241|\n",
      "|516.8782180531416| 435.4562057109809|\n",
      "|516.9301723033138| 434.8421577557409|\n",
      "|516.8058037183525| 435.6214462503029|\n",
      "|516.5456508321224|426.73458868118234|\n",
      "|516.3220107680502|445.34295413300896|\n",
      "|516.4141263941766|445.61465167269284|\n",
      "+-----------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Root Mean Squared Error (RMSE) on test data = 65.9148\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorIndexer\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Evaluate the model on training data\n",
    "prediction.select([\"dollars_saved\", \"prediction\"]).show()\n",
    "\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=label, predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(prediction)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2: Random Forest Regression\n",
    "Best hyperparameters: maxDepth: 8, numTrees: 250, minInstancesPerNode: 1\n",
    "\n",
    "Model Size: 4.9 MB\n",
    "\n",
    "RMSE: 33.744"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 33.744\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "rf = RandomForestRegressor(featuresCol=\"scaled_features\", labelCol=label, maxDepth=8, numTrees=250, minInstancesPerNode=1)\n",
    "\n",
    "# Chain indexer and forest in a Pipeline\n",
    "pipeline_rf = Pipeline(stages=[assembler_features, scaler, rf])\n",
    "\n",
    "model = pipeline_rf.fit(df_train)\n",
    "model.save(\"model-rf\")\n",
    "predictions_rf = model.transform(df_test)\n",
    "\n",
    "evaluator = RegressionEvaluator(\n",
    "   labelCol=label, predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions_rf)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+\n",
      "|    dollars_saved|       prediction|\n",
      "+-----------------+-----------------+\n",
      "|484.5031021125742| 516.505343590026|\n",
      "|484.6682391251149|517.4366364473047|\n",
      "|516.8557923471669|518.5480942673072|\n",
      "|516.0491487817686|518.4635486310632|\n",
      "|516.0173890931666|518.0718040583994|\n",
      "|515.7661149761013|516.8476291907117|\n",
      "|515.8825341130485|516.8290975153025|\n",
      "| 515.864331864375|518.1099441179163|\n",
      "|516.4802310303666|517.9523654638882|\n",
      "|515.7580085695923|518.1099441179163|\n",
      "|484.4272456962049| 516.254079042995|\n",
      "|516.0233877856182| 518.061149261007|\n",
      "|516.4444948751209|518.2245319764656|\n",
      "|516.3146217883103|518.1129351300027|\n",
      "|516.8782180531416|517.0463894341927|\n",
      "|516.9301723033138|513.7632972381936|\n",
      "|516.8058037183525|513.6393711245371|\n",
      "|516.5456508321224|519.4899607344098|\n",
      "|516.3220107680502|519.1714144267758|\n",
      "|516.4141263941766|519.1675502918422|\n",
      "+-----------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions_rf.select([\"dollars_saved\", \"prediction\"]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3: Gradient Boosted Trees\n",
    "Best hyperparameters: maxDepth:8, minInstancesPerNode:2, maxIter: 100, stepSize: 0.1\n",
    "\n",
    "Model Size: 1.7MB\n",
    "\n",
    "RMSE: 20.7213"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import GBTRegressor\n",
    "\n",
    "gbt = GBTRegressor(labelCol=label, predictionCol='prediction', featuresCol='scaled_features', maxDepth=8, minInstancesPerNode=2, maxIter=100, stepSize=0.1)\n",
    "\n",
    "# Chain indexer and forest in a Pipeline\n",
    "pipeline_gbt = Pipeline(stages=[assembler_features, scaler, gbt])\n",
    "\n",
    "# Train model.  This also runs the indexer.\n",
    "model = pipeline_gbt.fit(df_train)\n",
    "model.save(\"model-gbt\")\n",
    "# Make predictions.\n",
    "predictions_gbt = model.transform(df_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 20.7213\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=label, predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions_gbt)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------------------+\n",
      "|    dollars_saved|        prediction|\n",
      "+-----------------+------------------+\n",
      "|484.5031021125742| 506.8656149623041|\n",
      "|484.6682391251149| 517.2272117653333|\n",
      "|516.8557923471669| 513.6508706464688|\n",
      "|516.0491487817686| 518.2075095279591|\n",
      "|516.0173890931666| 516.7011605913955|\n",
      "|515.7661149761013|510.68275608126334|\n",
      "|515.8825341130485|511.71881025903707|\n",
      "| 515.864331864375| 516.6866065674952|\n",
      "|516.4802310303666| 512.2948439893455|\n",
      "|515.7580085695923| 517.1193681228697|\n",
      "|484.4272456962049| 507.3258785651495|\n",
      "|516.0233877856182| 516.0678735473616|\n",
      "|516.4444948751209| 515.7743016470491|\n",
      "|516.3146217883103| 516.7414089283822|\n",
      "|516.8782180531416| 513.5438512305753|\n",
      "|516.9301723033138| 513.9171039350417|\n",
      "|516.8058037183525| 514.1072459065782|\n",
      "|516.5456508321224| 515.8038068640749|\n",
      "|516.3220107680502|  520.957962688935|\n",
      "|516.4141263941766| 520.8632931932017|\n",
      "+-----------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions_gbt.select([\"dollars_saved\", \"prediction\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook /sfs/qumulo/qhome/keh4nb/ds5559_project/Solar Models.ipynb to pdf\n",
      "[NbConvertApp] Writing 42284 bytes to ./notebook.tex\n",
      "[NbConvertApp] Building PDF\n",
      "[NbConvertApp] Running xelatex 3 times: ['xelatex', './notebook.tex', '-quiet']\n",
      "[NbConvertApp] Running bibtex 1 time: ['bibtex', './notebook']\n",
      "[NbConvertApp] WARNING | bibtex had problems, most likely because there were no citations\n",
      "[NbConvertApp] PDF successfully created\n",
      "[NbConvertApp] Writing 50631 bytes to /sfs/qumulo/qhome/keh4nb/ds5559_project/Solar Models.pdf\n",
      "[NbConvertApp] Converting notebook /sfs/qumulo/qhome/keh4nb/ds5559_project/Solar Regression-Copy1.ipynb to pdf\n",
      "[NbConvertApp] Writing 59218 bytes to ./notebook.tex\n",
      "[NbConvertApp] Building PDF\n",
      "[NbConvertApp] Running xelatex 3 times: ['xelatex', './notebook.tex', '-quiet']\n",
      "[NbConvertApp] Running bibtex 1 time: ['bibtex', './notebook']\n",
      "[NbConvertApp] WARNING | bibtex had problems, most likely because there were no citations\n",
      "[NbConvertApp] PDF successfully created\n",
      "[NbConvertApp] Writing 55720 bytes to /sfs/qumulo/qhome/keh4nb/ds5559_project/Solar Regression-Copy1.pdf\n"
     ]
    }
   ],
   "source": [
    "# Save notebook as PDF document\n",
    "!jupyter nbconvert --to pdf `pwd`/*.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:deeplearning]",
   "language": "python",
   "name": "conda-env-deeplearning-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
