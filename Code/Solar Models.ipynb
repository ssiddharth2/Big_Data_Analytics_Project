{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 6.8 Model Building\n",
    "\n",
    "Nikki Aaron (na5zn), Kevin Hoffman (keh4nb), Ashley Scurlock (ams5zx), Siddharth Surapaneni (sss2ea)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[8]\") \\\n",
    "    .appName(\"solar\") \\\n",
    "    .config(\"spark.driver.memory\",'100G') \\\n",
    "    .config('spark.default.parallelism', 16)\\\n",
    "    .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sc.setCheckpointDir('./checkpoints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- zip: integer (nullable = true)\n",
      " |-- lat: double (nullable = true)\n",
      " |-- lng: double (nullable = true)\n",
      " |-- timezone: string (nullable = true)\n",
      " |-- county_name: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- state_id: string (nullable = true)\n",
      " |-- state_name: string (nullable = true)\n",
      " |-- population: double (nullable = true)\n",
      " |-- density: double (nullable = true)\n",
      " |-- res_rate: double (nullable = true)\n",
      " |-- annual_mbtu_used: double (nullable = true)\n",
      " |-- annual_kwh_used: double (nullable = true)\n",
      " |-- temp_Apr: double (nullable = true)\n",
      " |-- temp_Aug: double (nullable = true)\n",
      " |-- temp_Dec: double (nullable = true)\n",
      " |-- temp_Feb: double (nullable = true)\n",
      " |-- temp_Jan: double (nullable = true)\n",
      " |-- temp_Jul: double (nullable = true)\n",
      " |-- temp_Jun: double (nullable = true)\n",
      " |-- temp_Mar: double (nullable = true)\n",
      " |-- temp_May: double (nullable = true)\n",
      " |-- temp_Nov: double (nullable = true)\n",
      " |-- temp_Oct: double (nullable = true)\n",
      " |-- temp_Sep: double (nullable = true)\n",
      " |-- pct_cloudy_days_Apr: double (nullable = true)\n",
      " |-- pct_cloudy_days_Aug: double (nullable = true)\n",
      " |-- pct_cloudy_days_Dec: double (nullable = true)\n",
      " |-- pct_cloudy_days_Feb: double (nullable = true)\n",
      " |-- pct_cloudy_days_Jan: double (nullable = true)\n",
      " |-- pct_cloudy_days_Jul: double (nullable = true)\n",
      " |-- pct_cloudy_days_Jun: double (nullable = true)\n",
      " |-- pct_cloudy_days_Mar: double (nullable = true)\n",
      " |-- pct_cloudy_days_May: double (nullable = true)\n",
      " |-- pct_cloudy_days_Nov: double (nullable = true)\n",
      " |-- pct_cloudy_days_Oct: double (nullable = true)\n",
      " |-- pct_cloudy_days_Sep: double (nullable = true)\n",
      " |-- annual_output_w_hrs: double (nullable = true)\n",
      " |-- dni: double (nullable = true)\n",
      " |-- annual_output_kwh_20_sps: double (nullable = true)\n",
      " |-- percent_current_needs_met: double (nullable = true)\n",
      " |-- dollars_saved: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "# Load training data\n",
    "df = spark.read.csv(\"data_combined.csv\", inferSchema =True, header=True).repartition(500)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(lat=30.8494, lng=-86.20229, population=17877.0, density=26.6, annual_mbtu_used=56.2, annual_kwh_used=16470.594134000003, temp_Jan=31.4, pct_cloudy_days_Apr=53.1, pct_cloudy_days_Aug=63.6, pct_cloudy_days_Dec=63.15, pct_cloudy_days_Feb=71.05, pct_cloudy_days_Jan=61.150000000000006, pct_cloudy_days_Jul=66.15, pct_cloudy_days_Jun=57.85, pct_cloudy_days_Mar=61.95, pct_cloudy_days_May=58.55, pct_cloudy_days_Nov=51.7, pct_cloudy_days_Oct=53.400000000000006, pct_cloudy_days_Sep=53.6, dni=5.1744, annual_output_kwh_20_sps=3015.0274242205314, dollars_saved=381.2174559705672)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select columns for training\n",
    "label = \"dollars_saved\"\n",
    "feature_to_ignore = [\"res_rate\", \n",
    "                     \"zip\",\n",
    "                     \"annual_output_w_hrs\", \n",
    "                     \"anuual_output_kwh20_sps\", \n",
    "                     \"percent_current_needs_met\", \n",
    "                     \"country_name\", \n",
    "                     \"county_name\",\n",
    "                     \"city\", \n",
    "                     \"state_id\", \n",
    "                     \"state_name\", \n",
    "                     \"timezone\"]\n",
    "#                     \"temp_Apr\", \n",
    "#                     \"temp_Aug\",\n",
    "#                     \"temp_Dec\",\n",
    "#                     \"temp_Feb\",\n",
    "#                     \"temp_Jul\",\n",
    "#                     \"temp_Jun\",\n",
    "#                     \"temp_Mar\",\n",
    "#                     \"temp_May\",\n",
    "#                     \"temp_Nov\",\n",
    "#                     \"temp_Oct\",\n",
    "#                     \"temp_Sep\"]\n",
    "features = [c for c in df.columns if c not in feature_to_ignore + [label]]\n",
    "\n",
    "df_clean = df.select([c for c in df.columns if c not in feature_to_ignore])\n",
    "df_train, df_test = df_clean.randomSplit([0.8, 0.2], 500)\n",
    "df_train.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1: Linear Regression\n",
    "Best hyperparameters: maxIter:100, regParam:0, elasticNetParam: 0\n",
    "\n",
    "Model Size: 148 KB\n",
    "\n",
    "RMSE: 65.9148"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler, OneHotEncoder, StandardScaler\n",
    "from pyspark.ml import Pipeline \n",
    "\n",
    "assembler_features = VectorAssembler(\n",
    "    inputCols=features,\n",
    "    outputCol=\"features\")\n",
    "\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\")\n",
    "\n",
    "lr = LinearRegression(maxIter=100, labelCol=label, featuresCol=\"scaled_features\", regParam=0.0, elasticNetParam=0.0,)\n",
    "\n",
    "# Build the pipeline\n",
    "pipeline = Pipeline(stages=[assembler_features, scaler, lr])\n",
    "\n",
    "# Fit the pipeline\n",
    "model = pipeline.fit(df_train)\n",
    "#model.save(\"model-regression\")\n",
    "# Make a prediction\n",
    "prediction = model.transform(df_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+\n",
      "|     dollars_saved|        prediction|\n",
      "+------------------+------------------+\n",
      "| 309.3342204919779|293.61005185519537|\n",
      "| 382.2387730709182| 363.5218175714407|\n",
      "| 297.1028470491332| 337.7505596883436|\n",
      "|389.85693619720223|338.18061100140665|\n",
      "| 331.1423580819627| 341.3120479147233|\n",
      "|298.01351691675757|321.80796565206606|\n",
      "|288.61508982131323|299.90531201410283|\n",
      "| 380.0042629133563| 409.9592640269442|\n",
      "| 337.3079471963648| 369.1681453617781|\n",
      "|302.87126072752625|354.92004332185263|\n",
      "| 285.6261048310314| 363.8324451333126|\n",
      "| 450.1469332525246| 349.2711049103759|\n",
      "|298.41318348642153| 385.9261424771238|\n",
      "|289.95876586700865|  364.231389301843|\n",
      "|317.29404297577935|350.49328504293896|\n",
      "| 353.1318735873656| 351.9040495987658|\n",
      "| 321.9092395432814|349.04952188103834|\n",
      "| 579.7005971027172|496.69947483448783|\n",
      "|291.42452919195284| 288.2683382062047|\n",
      "| 313.9195139115664| 332.0051818076171|\n",
      "+------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Root Mean Squared Error (RMSE) on test data = 71.8452\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorIndexer\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Evaluate the model on training data\n",
    "prediction.select([\"dollars_saved\", \"prediction\"]).show()\n",
    "\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=label, predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(prediction)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2: Random Forest Regression\n",
    "Best hyperparameters: maxDepth: 8, numTrees: 250, minInstancesPerNode: 1\n",
    "\n",
    "Model Size: 4.9 MB\n",
    "\n",
    "RMSE: 33.744"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3331, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-6-3fcfcd1f81a0>\", line 7, in <module>\n",
      "    model = pipeline_rf.fit(df_train)\n",
      "  File \"/usr/local/spark/python/pyspark/ml/base.py\", line 132, in fit\n",
      "    return self._fit(dataset)\n",
      "  File \"/usr/local/spark/python/pyspark/ml/pipeline.py\", line 109, in _fit\n",
      "    model = stage.fit(dataset)\n",
      "  File \"/usr/local/spark/python/pyspark/ml/base.py\", line 132, in fit\n",
      "    return self._fit(dataset)\n",
      "  File \"/usr/local/spark/python/pyspark/ml/wrapper.py\", line 295, in _fit\n",
      "    java_model = self._fit_java(dataset)\n",
      "  File \"/usr/local/spark/python/pyspark/ml/wrapper.py\", line 292, in _fit_java\n",
      "    return self._java_obj.fit(dataset._jdf)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1255, in __call__\n",
      "    answer = self.gateway_client.send_command(command)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 985, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1152, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/opt/conda/lib/python3.7/socket.py\", line 589, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2044, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/genericpath.py\", line 19, in exists\n",
      "    os.stat(path)\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '<ipython-input-6-3fcfcd1f81a0>'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 1148, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 316, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 350, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/opt/conda/lib/python3.7/inspect.py\", line 1502, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/opt/conda/lib/python3.7/inspect.py\", line 1460, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/opt/conda/lib/python3.7/inspect.py\", line 693, in getsourcefile\n",
      "    if os.path.exists(filename):\n",
      "  File \"/opt/conda/lib/python3.7/genericpath.py\", line 19, in exists\n",
      "    os.stat(path)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "rf = RandomForestRegressor(featuresCol=\"scaled_features\", labelCol=label, maxDepth=8, numTrees=250, minInstancesPerNode=1)\n",
    "\n",
    "# Chain indexer and forest in a Pipeline\n",
    "pipeline_rf = Pipeline(stages=[assembler_features, scaler, rf])\n",
    "\n",
    "model = pipeline_rf.fit(df_train)\n",
    "#model.save(\"model-rf\")\n",
    "predictions_rf = model.transform(df_test)\n",
    "\n",
    "evaluator = RegressionEvaluator(\n",
    "   labelCol=label, predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions_rf)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+\n",
      "|    dollars_saved|       prediction|\n",
      "+-----------------+-----------------+\n",
      "|484.5031021125742| 516.505343590026|\n",
      "|484.6682391251149|517.4366364473047|\n",
      "|516.8557923471669|518.5480942673072|\n",
      "|516.0491487817686|518.4635486310632|\n",
      "|516.0173890931666|518.0718040583994|\n",
      "|515.7661149761013|516.8476291907117|\n",
      "|515.8825341130485|516.8290975153025|\n",
      "| 515.864331864375|518.1099441179163|\n",
      "|516.4802310303666|517.9523654638882|\n",
      "|515.7580085695923|518.1099441179163|\n",
      "|484.4272456962049| 516.254079042995|\n",
      "|516.0233877856182| 518.061149261007|\n",
      "|516.4444948751209|518.2245319764656|\n",
      "|516.3146217883103|518.1129351300027|\n",
      "|516.8782180531416|517.0463894341927|\n",
      "|516.9301723033138|513.7632972381936|\n",
      "|516.8058037183525|513.6393711245371|\n",
      "|516.5456508321224|519.4899607344098|\n",
      "|516.3220107680502|519.1714144267758|\n",
      "|516.4141263941766|519.1675502918422|\n",
      "+-----------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions_rf.select([\"dollars_saved\", \"prediction\"]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3: Gradient Boosted Trees\n",
    "Best hyperparameters: maxDepth:8, minInstancesPerNode:2, maxIter: 100, stepSize: 0.1\n",
    "\n",
    "Model Size: 1.7MB\n",
    "\n",
    "RMSE: 20.7213\n",
    "\n",
    "With no temp  maxDepth:8, minInstancesPerNode:4, maxIter: 200, stepSize: 0.1 : 20.029\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "gbt = GBTRegressor(labelCol=label, predictionCol='prediction', featuresCol='scaled_features', maxDepth=8, minInstancesPerNode=1, maxIter=150, stepSize=0.1)\n",
    "\n",
    "paramGrid = ParamGridBuilder()\\\n",
    "  .addGrid(gbt.minInstancesPerNode, [1, 2, 4])\\\n",
    "  .build()\n",
    "\n",
    "# We define an evaluation metric.  This tells CrossValidator how well we are doing by comparing the true labels with predictions.\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=gbt.getLabelCol(), predictionCol=gbt.getPredictionCol())\n",
    "# Declare the CrossValidator, which runs model tuning for us.\n",
    "cv = CrossValidator(estimator=gbt, evaluator=evaluator, estimatorParamMaps=paramGrid)\n",
    "# Chain indexer and forest in a Pipeline\n",
    "#pipeline_gbt = Pipeline(stages=[assembler_features, scaler, gbt])\n",
    "pipeline_gbt = Pipeline(stages=[assembler_features, scaler, cv])\n",
    "\n",
    "# Train model.  This also runs the indexer.\n",
    "model = pipeline_gbt.fit(df_train)\n",
    "display(model.bestModel)\n",
    "#model.save(\"model-gbt\")\n",
    "# Make predictions.\n",
    "predictions_gbt = model.transform(df_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=label, predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions_gbt)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------------------+\n",
      "|    dollars_saved|        prediction|\n",
      "+-----------------+------------------+\n",
      "|484.5031021125742| 506.8656149623041|\n",
      "|484.6682391251149| 517.2272117653333|\n",
      "|516.8557923471669| 513.6508706464688|\n",
      "|516.0491487817686| 518.2075095279591|\n",
      "|516.0173890931666| 516.7011605913955|\n",
      "|515.7661149761013|510.68275608126334|\n",
      "|515.8825341130485|511.71881025903707|\n",
      "| 515.864331864375| 516.6866065674952|\n",
      "|516.4802310303666| 512.2948439893455|\n",
      "|515.7580085695923| 517.1193681228697|\n",
      "|484.4272456962049| 507.3258785651495|\n",
      "|516.0233877856182| 516.0678735473616|\n",
      "|516.4444948751209| 515.7743016470491|\n",
      "|516.3146217883103| 516.7414089283822|\n",
      "|516.8782180531416| 513.5438512305753|\n",
      "|516.9301723033138| 513.9171039350417|\n",
      "|516.8058037183525| 514.1072459065782|\n",
      "|516.5456508321224| 515.8038068640749|\n",
      "|516.3220107680502|  520.957962688935|\n",
      "|516.4141263941766| 520.8632931932017|\n",
      "+-----------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions_gbt.select([\"dollars_saved\", \"prediction\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook /sfs/qumulo/qhome/keh4nb/ds5559_project/Solar Models.ipynb to pdf\n",
      "[NbConvertApp] Writing 42283 bytes to ./notebook.tex\n",
      "[NbConvertApp] Building PDF\n",
      "[NbConvertApp] Running xelatex 3 times: ['xelatex', './notebook.tex', '-quiet']\n",
      "[NbConvertApp] Running bibtex 1 time: ['bibtex', './notebook']\n",
      "[NbConvertApp] WARNING | bibtex had problems, most likely because there were no citations\n",
      "[NbConvertApp] PDF successfully created\n",
      "[NbConvertApp] Writing 50554 bytes to /sfs/qumulo/qhome/keh4nb/ds5559_project/Solar Models.pdf\n",
      "[NbConvertApp] Converting notebook /sfs/qumulo/qhome/keh4nb/ds5559_project/Solar Regression-Copy1.ipynb to pdf\n",
      "[NbConvertApp] Writing 59218 bytes to ./notebook.tex\n",
      "[NbConvertApp] Building PDF\n",
      "[NbConvertApp] Running xelatex 3 times: ['xelatex', './notebook.tex', '-quiet']\n",
      "[NbConvertApp] Running bibtex 1 time: ['bibtex', './notebook']\n",
      "[NbConvertApp] WARNING | bibtex had problems, most likely because there were no citations\n",
      "[NbConvertApp] PDF successfully created\n",
      "[NbConvertApp] Writing 55714 bytes to /sfs/qumulo/qhome/keh4nb/ds5559_project/Solar Regression-Copy1.pdf\n"
     ]
    }
   ],
   "source": [
    "# Save notebook as PDF document\n",
    "!jupyter nbconvert --to pdf `pwd`/*.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS 5559",
   "language": "python",
   "name": "ds5559"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
