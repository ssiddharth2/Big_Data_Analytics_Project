{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "expired-register",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "spark =SparkSession\\\n",
    "    .builder\\\n",
    "    .master(\"local[16]\")\\\n",
    "    .appName(\"solar\")\\\n",
    "    .config(\"spark.driver.memory\",'200G')\\\n",
    "    .config('spark.default.parallelism', '16')\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "indie-burlington",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- zip: integer (nullable = true)\n",
      " |-- lat: double (nullable = true)\n",
      " |-- lng: double (nullable = true)\n",
      " |-- timezone: string (nullable = true)\n",
      " |-- county_name: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- state_id: string (nullable = true)\n",
      " |-- state_name: string (nullable = true)\n",
      " |-- county_fips_all: string (nullable = true)\n",
      " |-- population: double (nullable = true)\n",
      " |-- density: double (nullable = true)\n",
      " |-- elevation: double (nullable = true)\n",
      " |-- dni: double (nullable = true)\n",
      " |-- res_rate: double (nullable = true)\n",
      " |-- annual_kwh_used: double (nullable = true)\n",
      " |-- temp_Apr: double (nullable = true)\n",
      " |-- temp_Aug: double (nullable = true)\n",
      " |-- temp_Dec: double (nullable = true)\n",
      " |-- temp_Feb: double (nullable = true)\n",
      " |-- temp_Jan: double (nullable = true)\n",
      " |-- temp_Jul: double (nullable = true)\n",
      " |-- temp_Jun: double (nullable = true)\n",
      " |-- temp_Mar: double (nullable = true)\n",
      " |-- temp_May: double (nullable = true)\n",
      " |-- temp_Nov: double (nullable = true)\n",
      " |-- temp_Oct: double (nullable = true)\n",
      " |-- temp_Sep: double (nullable = true)\n",
      " |-- pct_cloudy_days_Apr: double (nullable = true)\n",
      " |-- pct_cloudy_days_Aug: double (nullable = true)\n",
      " |-- pct_cloudy_days_Dec: double (nullable = true)\n",
      " |-- pct_cloudy_days_Feb: double (nullable = true)\n",
      " |-- pct_cloudy_days_Jan: double (nullable = true)\n",
      " |-- pct_cloudy_days_Jul: double (nullable = true)\n",
      " |-- pct_cloudy_days_Jun: double (nullable = true)\n",
      " |-- pct_cloudy_days_Mar: double (nullable = true)\n",
      " |-- pct_cloudy_days_May: double (nullable = true)\n",
      " |-- pct_cloudy_days_Nov: double (nullable = true)\n",
      " |-- pct_cloudy_days_Oct: double (nullable = true)\n",
      " |-- pct_cloudy_days_Sep: double (nullable = true)\n",
      " |-- temp_avg: double (nullable = true)\n",
      " |-- pct_cloudy_days_avg: double (nullable = true)\n",
      " |-- annual_output_w_hrs: double (nullable = true)\n",
      " |-- annual_output_w_hrs_30_panels: double (nullable = true)\n",
      " |-- dollars_saved: double (nullable = true)\n",
      " |-- percent_current_needs_met: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"final_data.csv\", inferSchema =True, header=True)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "certain-adelaide",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = \"dollars_saved\"\n",
    "\n",
    "feature_to_ignore = [\"res_rate\", \"annual_output_w_hrs\", \"anuual_output_kwh20_sps\",\"annual_output_w_hrs_30_panels\",\"county_fips_all\", \"percent_current_needs_met\", \"country_name\", \"county_name\", \"city\", \"state_id\", \"state_name\", \"timezone\"]\n",
    "features = [c for c in df.columns if c not in feature_to_ignore + [label]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "described-export",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['zip',\n",
       " 'lat',\n",
       " 'lng',\n",
       " 'population',\n",
       " 'density',\n",
       " 'elevation',\n",
       " 'dni',\n",
       " 'annual_kwh_used',\n",
       " 'temp_Apr',\n",
       " 'temp_Aug',\n",
       " 'temp_Dec',\n",
       " 'temp_Feb',\n",
       " 'temp_Jan',\n",
       " 'temp_Jul',\n",
       " 'temp_Jun',\n",
       " 'temp_Mar',\n",
       " 'temp_May',\n",
       " 'temp_Nov',\n",
       " 'temp_Oct',\n",
       " 'temp_Sep',\n",
       " 'pct_cloudy_days_Apr',\n",
       " 'pct_cloudy_days_Aug',\n",
       " 'pct_cloudy_days_Dec',\n",
       " 'pct_cloudy_days_Feb',\n",
       " 'pct_cloudy_days_Jan',\n",
       " 'pct_cloudy_days_Jul',\n",
       " 'pct_cloudy_days_Jun',\n",
       " 'pct_cloudy_days_Mar',\n",
       " 'pct_cloudy_days_May',\n",
       " 'pct_cloudy_days_Nov',\n",
       " 'pct_cloudy_days_Oct',\n",
       " 'pct_cloudy_days_Sep',\n",
       " 'temp_avg',\n",
       " 'pct_cloudy_days_avg']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "loaded-desktop",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(zip=1001, lat=42.06259, lng=-72.62589, timezone='America/New_York', county_name='Hampden', city='Agawam', state_id='MA', state_name='Massachusetts', county_fips_all='25013', population=17312.0, density=581.0, elevation=96.84615384615384, dni=4.776, res_rate=0.1592524036541533, annual_kwh_used=18463.47741, temp_Apr=8.15, temp_Aug=22.75, temp_Dec=-1.15, temp_Feb=-0.3999999999999999, temp_Jan=-4.5, temp_Jul=24.0, temp_Jun=19.75, temp_Mar=1.75, temp_May=16.0, temp_Nov=3.0, temp_Oct=11.2, temp_Sep=18.4, pct_cloudy_days_Apr=70.80000000000001, pct_cloudy_days_Aug=64.85, pct_cloudy_days_Dec=71.65, pct_cloudy_days_Feb=69.15, pct_cloudy_days_Jan=65.3, pct_cloudy_days_Jul=64.25, pct_cloudy_days_Jun=67.19999999999999, pct_cloudy_days_Mar=74.1, pct_cloudy_days_May=70.35, pct_cloudy_days_Nov=74.69999999999999, pct_cloudy_days_Oct=67.25, pct_cloudy_days_Sep=63.65, temp_avg=9.912499999999998, pct_cloudy_days_avg=68.60416666666667, annual_output_w_hrs=168998.8814399332, annual_output_w_hrs_30_panels=5069966.443197996, label=807.4043425251792, percent_current_needs_met=0.2745943426914831)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean = df.select([c for c in df.columns if c not in feature_to_ignore])\n",
    "df_clean=df.withColumnRenamed(label,\"label\")\n",
    "df_train, df_test = df_clean.randomSplit([0.8, 0.2], 500)\n",
    "df_train=df_train\n",
    "df_train.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "theoretical-pendant",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import PCA\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "hearing-maldives",
   "metadata": {},
   "outputs": [],
   "source": [
    "the_features=[c for c in df.columns if \"temp_\" in c or \"pct_\" in c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "informal-treasure",
   "metadata": {},
   "outputs": [],
   "source": [
    "other_features=[c for c in features if c not in the_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "first-discretion",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['zip',\n",
       " 'lat',\n",
       " 'lng',\n",
       " 'population',\n",
       " 'density',\n",
       " 'elevation',\n",
       " 'dni',\n",
       " 'annual_kwh_used']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "other_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "loving-venice",
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=[c for c in df.columns if \"temp_\" in c or \"pct_\" in c], outputCol=\"temp_pct_features\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "soviet-thanksgiving",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "scaler=StandardScaler(withMean=True,inputCol=\"temp_pct_features\", outputCol=\"scaled_temp_pct_Features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "preliminary-neutral",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA( inputCol=\"temp_pct_features\", outputCol=\"pcaFeatures\")\n",
    "#result.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "liberal-strand",
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler2 = VectorAssembler(inputCols=other_features, outputCol=\"other_features\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "stuck-germany",
   "metadata": {},
   "outputs": [],
   "source": [
    "other_scaler=StandardScaler(withMean=True,inputCol=\"other_features\", outputCol=\"scaled_other_Features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "swedish-batman",
   "metadata": {},
   "outputs": [],
   "source": [
    "va = VectorAssembler(inputCols=[\"pcaFeatures\", \"scaled_other_Features\"], outputCol=\"model_Features\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "union-investigation",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "closed-lodge",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression(maxIter=100,featuresCol='model_Features', labelCol=\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "apart-warehouse",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "republican-qualification",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[assembler, scaler, pca,assembler2,other_scaler,va,lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informed-orbit",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "offensive-accident",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "chief-onion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(paramGrid): 45\n",
      "train time: 75.36701607704163\n"
     ]
    }
   ],
   "source": [
    "# Set up the parameter grid\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(pca.k, [1,2,3, 6, 12]) \\\n",
    "    .addGrid(lr.regParam, [0.1, 0.3,0.01]) \\\n",
    "    .addGrid(lr.elasticNetParam, [0.2, 0.5,0.8]) \\\n",
    "    .build()\n",
    "\n",
    "print('len(paramGrid): {}'.format(len(paramGrid)))\n",
    "\n",
    "# Treat the Pipeline as an Estimator, wrapping it in a CrossValidator instance.\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=RegressionEvaluator(),\n",
    "                          numFolds=5)\n",
    "\n",
    "# Run cross-validation, and choose the best set of parameters. Print the training time.\n",
    "import time\n",
    "t0 = time.time()\n",
    "cvModel = crossval.setParallelism(4).fit(df_train) # train 4 models in parallel\n",
    "print(\"train time:\", time.time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "third-specific",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = cvModel.transform(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "chicken-gardening",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13777.38681565987"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1_MSE=prediction.rdd.map(lambda x: (x.label-x.prediction)**2).reduce(lambda x,y: x+y)/prediction.count()\n",
    "model1_MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "manufactured-coordinate",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "abstract-syria",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_assembler = VectorAssembler(inputCols=features, outputCol=\"features\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "legitimate-richmond",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(featuresCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "crazy-maximum",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_pipeline = Pipeline(stages=[rf_assembler,rf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "still-conservative",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(paramGrid): 1\n",
      "train time: 44.93319225311279\n"
     ]
    }
   ],
   "source": [
    "rf_paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(rf.numTrees, [200]) \\\n",
    "    .addGrid(rf.maxDepth, [7]) \\\n",
    "    .build()\n",
    "\n",
    "print('len(paramGrid): {}'.format(len(rf_paramGrid)))\n",
    "\n",
    "# Treat the Pipeline as an Estimator, wrapping it in a CrossValidator instance.\n",
    "crossval = CrossValidator(estimator=rf_pipeline,\n",
    "                          estimatorParamMaps=rf_paramGrid,\n",
    "                          evaluator=RegressionEvaluator(),\n",
    "                          numFolds=5)\n",
    "\n",
    "# Run cross-validation, and choose the best set of parameters. Print the training time.\n",
    "import time\n",
    "t0 = time.time()\n",
    "cvModel_rf = crossval.setParallelism(8).fit(df_train) # train 4 models in parallel\n",
    "print(\"train time:\", time.time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "classical-danger",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3789.5524582743283"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_rf = cvModel_rf.transform(df_test)\n",
    "model2_MSE=prediction_rf.rdd.map(lambda x: (x.label-x.prediction)**2).reduce(lambda x,y: x+y)/prediction_rf.count()\n",
    "model2_MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "informative-madagascar",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(paramGrid): 1\n",
      "train time: 55.75451421737671\n"
     ]
    }
   ],
   "source": [
    "rf_paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(rf.numTrees, [250]) \\\n",
    "    .addGrid(rf.maxDepth, [7]) \\\n",
    "    .build()\n",
    "\n",
    "print('len(paramGrid): {}'.format(len(rf_paramGrid)))\n",
    "\n",
    "# Treat the Pipeline as an Estimator, wrapping it in a CrossValidator instance.\n",
    "crossval = CrossValidator(estimator=rf_pipeline,\n",
    "                          estimatorParamMaps=rf_paramGrid,\n",
    "                          evaluator=RegressionEvaluator(),\n",
    "                          numFolds=5)\n",
    "\n",
    "# Run cross-validation, and choose the best set of parameters. Print the training time.\n",
    "import time\n",
    "t0 = time.time()\n",
    "cvModel_rf = crossval.setParallelism(8).fit(df_train) # train 4 models in parallel\n",
    "print(\"train time:\", time.time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "urban-breeding",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3739.9367090812552"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_rf = cvModel_rf.transform(df_test)\n",
    "model2_1_MSE=prediction_rf.rdd.map(lambda x: (x.label-x.prediction)**2).reduce(lambda x,y: x+y)/prediction_rf.count()\n",
    "model2_1_MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "arctic-husband",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import GBTRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "assigned-potter",
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt = GBTRegressor(featuresCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "technical-stockholm",
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt_pipeline = Pipeline(stages=[rf_assembler,gbt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "adequate-privacy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(paramGrid): 1\n",
      "train time: 187.19522190093994\n"
     ]
    }
   ],
   "source": [
    "gbt_paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(gbt.maxIter, [200]) \\\n",
    "    .addGrid(gbt.maxDepth, [3]) \\\n",
    "    .build()\n",
    "\n",
    "print('len(paramGrid): {}'.format(len(gbt_paramGrid)))\n",
    "\n",
    "# Treat the Pipeline as an Estimator, wrapping it in a CrossValidator instance.\n",
    "crossval_gbt = CrossValidator(estimator=gbt_pipeline,\n",
    "                          estimatorParamMaps=gbt_paramGrid,\n",
    "                          evaluator=RegressionEvaluator(),\n",
    "                          numFolds=5)\n",
    "\n",
    "# Run cross-validation, and choose the best set of parameters. Print the training time.\n",
    "import time\n",
    "t0 = time.time()\n",
    "cvModel_gbt = crossval_gbt.setParallelism(16).fit(df_train) # train 4 models in parallel\n",
    "print(\"train time:\", time.time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "threatened-variance",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2188.30982639766"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_gbt = cvModel_gbt.transform(df_test)\n",
    "model3_MSE=prediction_gbt.rdd.map(lambda x: (x.label-x.prediction)**2).reduce(lambda x,y: x+y)/prediction_gbt.count()\n",
    "model3_MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "according-salon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(paramGrid): 1\n",
      "train time: 250.116375207901\n"
     ]
    }
   ],
   "source": [
    "gbt_paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(gbt.maxIter, [200]) \\\n",
    "    .addGrid(gbt.maxDepth, [4]) \\\n",
    "    .build()\n",
    "\n",
    "print('len(paramGrid): {}'.format(len(gbt_paramGrid)))\n",
    "\n",
    "# Treat the Pipeline as an Estimator, wrapping it in a CrossValidator instance.\n",
    "crossval_gbt = CrossValidator(estimator=gbt_pipeline,\n",
    "                          estimatorParamMaps=gbt_paramGrid,\n",
    "                          evaluator=RegressionEvaluator(),\n",
    "                          numFolds=5)\n",
    "\n",
    "# Run cross-validation, and choose the best set of parameters. Print the training time.\n",
    "import time\n",
    "t0 = time.time()\n",
    "cvModel_gbt = crossval_gbt.setParallelism(16).fit(df_train) # train 4 models in parallel\n",
    "print(\"train time:\", time.time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "necessary-raleigh",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1619.0349551608672"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_gbt = cvModel_gbt.transform(df_test)\n",
    "model4_MSE=prediction_gbt.rdd.map(lambda x: (x.label-x.prediction)**2).reduce(lambda x,y: x+y)/prediction_gbt.count()\n",
    "model4_MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cellular-alarm",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(paramGrid): 1\n",
      "train time: 341.1487011909485\n"
     ]
    }
   ],
   "source": [
    "gbt_paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(gbt.maxIter, [200]) \\\n",
    "    .addGrid(gbt.maxDepth, [5]) \\\n",
    "    .build()\n",
    "\n",
    "print('len(paramGrid): {}'.format(len(gbt_paramGrid)))\n",
    "\n",
    "# Treat the Pipeline as an Estimator, wrapping it in a CrossValidator instance.\n",
    "crossval_gbt = CrossValidator(estimator=gbt_pipeline,\n",
    "                          estimatorParamMaps=gbt_paramGrid,\n",
    "                          evaluator=RegressionEvaluator(),\n",
    "                          numFolds=5)\n",
    "\n",
    "# Run cross-validation, and choose the best set of parameters. Print the training time.\n",
    "import time\n",
    "t0 = time.time()\n",
    "cvModel_gbt = crossval_gbt.setParallelism(16).fit(df_train) # train 4 models in parallel\n",
    "print(\"train time:\", time.time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "turned-regulation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1350.7579238804585"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_gbt = cvModel_gbt.transform(df_test)\n",
    "model5_MSE=prediction_gbt.rdd.map(lambda x: (x.label-x.prediction)**2).reduce(lambda x,y: x+y)/prediction_gbt.count()\n",
    "model5_MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "protecting-fifth",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(paramGrid): 1\n",
      "train time: 402.874489068985\n"
     ]
    }
   ],
   "source": [
    "gbt_paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(gbt.maxIter, [225]) \\\n",
    "    .addGrid(gbt.maxDepth, [5]) \\\n",
    "    .build()\n",
    "\n",
    "print('len(paramGrid): {}'.format(len(gbt_paramGrid)))\n",
    "\n",
    "# Treat the Pipeline as an Estimator, wrapping it in a CrossValidator instance.\n",
    "crossval_gbt = CrossValidator(estimator=gbt_pipeline,\n",
    "                          estimatorParamMaps=gbt_paramGrid,\n",
    "                          evaluator=RegressionEvaluator(),\n",
    "                          numFolds=5)\n",
    "\n",
    "# Run cross-validation, and choose the best set of parameters. Print the training time.\n",
    "import time\n",
    "t0 = time.time()\n",
    "cvModel_gbt = crossval_gbt.setParallelism(16).fit(df_train) # train 4 models in parallel\n",
    "print(\"train time:\", time.time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "verbal-psychiatry",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1328.5188606474012"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_gbt = cvModel_gbt.transform(df_test)\n",
    "model6_MSE=prediction_gbt.rdd.map(lambda x: (x.label-x.prediction)**2).reduce(lambda x,y: x+y)/prediction_gbt.count()\n",
    "model6_MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "guided-mountain",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(paramGrid): 1\n",
      "train time: 1008.6885416507721\n"
     ]
    }
   ],
   "source": [
    "gbt_paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(gbt.maxIter, [225]) \\\n",
    "    .addGrid(gbt.maxDepth, [7]) \\\n",
    "    .build()\n",
    "\n",
    "print('len(paramGrid): {}'.format(len(gbt_paramGrid)))\n",
    "\n",
    "# Treat the Pipeline as an Estimator, wrapping it in a CrossValidator instance.\n",
    "crossval_gbt = CrossValidator(estimator=gbt_pipeline,\n",
    "                          estimatorParamMaps=gbt_paramGrid,\n",
    "                          evaluator=RegressionEvaluator(),\n",
    "                          numFolds=5)\n",
    "\n",
    "# Run cross-validation, and choose the best set of parameters. Print the training time.\n",
    "import time\n",
    "t0 = time.time()\n",
    "cvModel_gbt = crossval_gbt.setParallelism(16).fit(df_train) # train 4 models in parallel\n",
    "print(\"train time:\", time.time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dated-cornwall",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1239.8791954032304"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_gbt = cvModel_gbt.transform(df_test)\n",
    "model7_MSE=prediction_gbt.rdd.map(lambda x: (x.label-x.prediction)**2).reduce(lambda x,y: x+y)/prediction_gbt.count()\n",
    "model7_MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "played-spencer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(paramGrid): 1\n",
      "train time: 2523.4385073184967\n"
     ]
    }
   ],
   "source": [
    "gbt_paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(gbt.maxIter, [225]) \\\n",
    "    .addGrid(gbt.maxDepth, [9]) \\\n",
    "    .build()\n",
    "\n",
    "print('len(paramGrid): {}'.format(len(gbt_paramGrid)))\n",
    "\n",
    "# Treat the Pipeline as an Estimator, wrapping it in a CrossValidator instance.\n",
    "crossval_gbt = CrossValidator(estimator=gbt_pipeline,\n",
    "                          estimatorParamMaps=gbt_paramGrid,\n",
    "                          evaluator=RegressionEvaluator(),\n",
    "                          numFolds=5)\n",
    "\n",
    "# Run cross-validation, and choose the best set of parameters. Print the training time.\n",
    "import time\n",
    "t0 = time.time()\n",
    "cvModel_gbt = crossval_gbt.setParallelism(16).fit(df_train) # train 4 models in parallel\n",
    "print(\"train time:\", time.time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "moderate-addiction",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1402.4429009027965"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_gbt = cvModel_gbt.transform(df_test)\n",
    "model8_MSE=prediction_gbt.rdd.map(lambda x: (x.label-x.prediction)**2).reduce(lambda x,y: x+y)/prediction_gbt.count()\n",
    "model8_MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "german-xerox",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(paramGrid): 1\n",
      "train time: 741.2755110263824\n"
     ]
    }
   ],
   "source": [
    "gbt_paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(gbt.maxIter, [225]) \\\n",
    "    .addGrid(gbt.minInstancesPerNode, [10])\\\n",
    "    .addGrid(gbt.maxDepth, [7]) \\\n",
    "    .build()\n",
    "\n",
    "print('len(paramGrid): {}'.format(len(gbt_paramGrid)))\n",
    "\n",
    "# Treat the Pipeline as an Estimator, wrapping it in a CrossValidator instance.\n",
    "crossval_gbt = CrossValidator(estimator=gbt_pipeline,\n",
    "                          estimatorParamMaps=gbt_paramGrid,\n",
    "                          evaluator=RegressionEvaluator(),\n",
    "                          numFolds=5)\n",
    "\n",
    "# Run cross-validation, and choose the best set of parameters. Print the training time.\n",
    "import time\n",
    "t0 = time.time()\n",
    "cvModel_gbt = crossval_gbt.setParallelism(16).fit(df_train) # train 4 models in parallel\n",
    "print(\"train time:\", time.time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "suitable-antigua",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1245.6296355317386"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_gbt = cvModel_gbt.transform(df_test)\n",
    "model9_MSE=prediction_gbt.rdd.map(lambda x: (x.label-x.prediction)**2).reduce(lambda x,y: x+y)/prediction_gbt.count()\n",
    "model9_MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "appropriate-madrid",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(paramGrid): 1\n",
      "train time: 760.796703338623\n"
     ]
    }
   ],
   "source": [
    "gbt_paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(gbt.maxIter, [225]) \\\n",
    "    .addGrid(gbt.minInstancesPerNode, [8])\\\n",
    "    .addGrid(gbt.maxDepth, [7]) \\\n",
    "    .build()\n",
    "\n",
    "print('len(paramGrid): {}'.format(len(gbt_paramGrid)))\n",
    "\n",
    "# Treat the Pipeline as an Estimator, wrapping it in a CrossValidator instance.\n",
    "crossval_gbt = CrossValidator(estimator=gbt_pipeline,\n",
    "                          estimatorParamMaps=gbt_paramGrid,\n",
    "                          evaluator=RegressionEvaluator(),\n",
    "                          numFolds=5)\n",
    "\n",
    "# Run cross-validation, and choose the best set of parameters. Print the training time.\n",
    "import time\n",
    "t0 = time.time()\n",
    "cvModel_gbt = crossval_gbt.setParallelism(16).fit(df_train) # train 4 models in parallel\n",
    "print(\"train time:\", time.time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "worthy-visitor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1256.5841938024962"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_gbt = cvModel_gbt.transform(df_test)\n",
    "model10_MSE=prediction_gbt.rdd.map(lambda x: (x.label-x.prediction)**2).reduce(lambda x,y: x+y)/prediction_gbt.count()\n",
    "model10_MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controlled-eclipse",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS 5559 Spark 3",
   "language": "python",
   "name": "ds5559_spark3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
